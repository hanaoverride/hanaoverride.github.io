---
layout: post
title: "Attention Is All You Need ë…¼ë¬¸ : ì²˜ìŒ ì½ì–´ë³´ê¸°! - (1)"
date: 2025-02-20 18:20:24 +0900
categories: [llm-engineering]
tags: [Transformer, ë…¼ë¬¸ë¦¬ë·°, AttentionMechanism, ë”¥ëŸ¬ë‹]
---

### **Part 1: "ì™œ Transformerì¸ê°€?" - RNNì˜ í•œê³„ì™€ Attentionì˜ ë“±ì¥**


## ğŸ¤” ì´ ë…¼ë¬¸, ì™œ ëª¨ë‘ê°€ ë– ë“¤ì©í•œê°€?

"Attention Is All You Need"... ì œëª©ë¶€í„° ë­”ê°€ ë„ë°œì ì´ì£ ? 
2017ë…„ì— ë‚˜ì˜¨ ì´ ë…¼ë¬¸ í•˜ë‚˜ê°€ AI íŒë„ë¥¼ ì™„ì „íˆ ë°”ê¿”ë†¨ìŠµë‹ˆë‹¤. ChatGPT, Claude, Gemini... ìš”ì¦˜ í•«í•œ AIë“¤ ì „ë¶€ ì´ ë…¼ë¬¸ì—ì„œ ì‹œì‘ëê±°ë“ ìš”. í˜„ëŒ€ LLM ì•„í‚¤í…ì³ëŠ” ëŒ€ë¶€ë¶„ ì´ Transformer ê¸°ë°˜ì´ê±°ë‚˜ Transformer ë³€í˜• ì•„í‚¤í…ì³ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. LLMì„ ì´í•´í•˜ê³  ì‹¶ë‹¤ë©´ ì •ë§ì •ë§ Transformerë§Œí¼ì€ ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤.

(Diffusion ë“± ë‹¤ë¥¸ ê¸°ìˆ  ê¸°ë°˜ ëª¨ë¸ë„ ìˆì§€ë§Œ, ì˜¤ëŠ˜ì€ Transformerì— ì§‘ì¤‘í• ê²Œìš”.)

ê·¼ë° ì†”ì§íˆ ë§í•˜ë©´, ë…¼ë¬¸ ì½ê¸° ì‰½ì§€ ì•Šì£ ? ì €ë„ ì²˜ìŒ ì½ì„ ë•Œ "ì•„ ì´ê²Œ ë­” ì†Œë¦¬ì•¼..." í–ˆìŠµë‹ˆë‹¤.
ê·¸ë˜ì„œ **í•¨ê»˜ ì½ì–´ë³´ë ¤ê³  í•©ë‹ˆë‹¤.** í˜¼ì ì½ìœ¼ë©´ ì–´ë ¤ìš´ë°, ê°™ì´ ì½ìœ¼ë©´ ì¢€ ëœ ë¶€ë‹´ìŠ¤ëŸ½ì–ì•„ìš”?

> Disclaimer: ì´ ë…¼ë¬¸ì€ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì— ëŒ€í•œ ê¸°ë³¸ ì§€ì‹ì€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. NLP ê´€ë ¨ ì§€ì‹ì€ ì—†ì–´ë„ ì½ì„ìˆ˜ ìˆì§€ë§Œ, ë”¥ëŸ¬ë‹ ê´€ë ¨ ì§€ì‹ì´ ì—†ë‹¤ë©´ ë¬´ìŠ¨ ë§ì¸ì§€ ê±°ì˜ ì´í•´í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì œ ê¹ƒí—ˆë¸Œ ë ˆí¬ì§€í† ë¦¬ì˜ [ë°ì´í„° ê³¼í•™ìë¥¼ ìœ„í•œ ì¿¡ë¶](https://github.com/hanaoverride/data-scientist-cookbook-for-korean) ì„ ì°¸ê³ í•˜ì„¸ìš”.

## ğŸ“– Introduction: "RNNì•„, ë„ˆ ì •ë§ ìµœì„ ì´ì—ˆë‹ˆ?"

ë…¼ë¬¸ì´ ì‹œì‘í•˜ìë§ˆì **RNNì„ ëŒ€ë†“ê³  ê¹Œê¸° ì‹œì‘í•©ë‹ˆë‹¤.**

> "This inherently sequential nature **precludes** parallelization within training examples, which **becomes critical at longer sequence lengths**"

ì‰½ê²Œ ë§í•˜ë©´ ì´ê²ë‹ˆë‹¤:
- RNN: "ë‚˜ëŠ” ë‹¨ì–´ë¥¼ í•˜ë‚˜ì”© ì°¨ë¡€ëŒ€ë¡œ ì²˜ë¦¬í•´ì•¼ í•´... ğŸ˜“"
- ìš°ë¦¬: "ì•„ë‹ˆ í˜•, ì§€ê¸ˆ GPU 100ê°œ ë†€ê³  ìˆëŠ”ë° ë³‘ë ¬ì²˜ë¦¬ ì¢€ í•˜ì"
- RNN: "ì•ˆë¼... ë‚˜ëŠ” ìˆœì„œëŒ€ë¡œë§Œ í•  ìˆ˜ ìˆì–´..."

**ì‹¤ì œë¡œ ì œê°€ ê²ªì—ˆë˜ ì¼ì¸ë°ìš”,** ì €ë„ ì˜›ë‚  ëª¨ë¸ ê³µë¶€ ì¢€ í•´ë³´ë ¤ê³  LSTMìœ¼ë¡œ ë²ˆì—­ ëª¨ë¸ ëŒë ¤ë´¤ë”ë‹ˆ ë¬¸ì¥ ê¸¸ì–´ì§ˆìˆ˜ë¡ í•™ìŠµ ì‹œê°„ì´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ëŠ˜ì–´ë‚˜ë”ë¼ê³ ìš”. 100ë‹¨ì–´ì§œë¦¬ ë¬¸ì¥? ê·¸ëƒ¥ í¬ê¸°í–ˆìŠµë‹ˆë‹¤.

## ğŸ’¡ ê·¸ë˜ì„œ ë­ê°€ ë‹¤ë¥¸ë°?: Attentionì˜ í˜ëª…

Transformerê°€ ì œì•ˆí•œ í•´ë²•ì€ ê°„ë‹¨í•©ë‹ˆë‹¤:

> "The Transformer allows for significantly more parallelization and can reach a new **state of the art** in translation quality **after being trained for as little as twelve hours on eight P100 GPUs.**"

**12ì‹œê°„ë§Œì— SOTA ë‹¬ì„±ì´ë¼ê³ ìš”?** 
ë‹¹ì‹œ ë‹¤ë¥¸ ëª¨ë¸ë“¤ì€ ë©°ì¹ ì”© ëŒë ¤ì•¼ í–ˆê±°ë“ ìš”. 

ë¹„ìœ í•˜ìë©´ ì´ëŸ° ê²ë‹ˆë‹¤:
- **ê¸°ì¡´ RNN**: ì±…ì„ ì²˜ìŒë¶€í„° ëê¹Œì§€ í•œ ê¸€ìì”© ì½ëŠ” ì‚¬ëŒ
- **Transformer**: ì±… ì „ì²´ë¥¼ í•œë²ˆì— í¼ì³ë†“ê³  ì¤‘ìš”í•œ ë¶€ë¶„ë¼ë¦¬ ì—°ê²°ì„  ê·¸ìœ¼ë©° ì½ëŠ” ì‚¬ëŒ

ì–´ë–¤ ê²Œ ë” íš¨ìœ¨ì ì¼ê¹Œìš”? ë‹¹ì—°íˆ í›„ìì£ .

## ğŸ” Self-Attention: "ë‚˜ ìì‹ ì„ ëŒì•„ë³´ë‹¤"

ë…¼ë¬¸ì—ì„œ í•µì‹¬ ê°œë…ì¸ Self-Attentionì„ ì´ë ‡ê²Œ ì„¤ëª…í•©ë‹ˆë‹¤:

> "**Self-attention**, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence."

ìŒ... ì¢€ ì¶”ìƒì ì´ì£ ? **ì‹¤ì œ ì˜ˆì‹œë¡œ ë´…ì‹œë‹¤:**

```
ë¬¸ì¥: "ì€í–‰ì— ê°€ì„œ ëˆì„ ì°¾ì•˜ë‹¤"
```

"ì€í–‰"ì´ë¼ëŠ” ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ë ¤ë©´?
- "ëˆ"ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ ë´ì•¼ í•¨ â†’ ì•„, ê¸ˆìœµê¸°ê´€ì´êµ¬ë‚˜! (ê°•ë‘‘ X)
- ëª¨ë“  ë‹¨ì–´ê°€ ì„œë¡œë¥¼ "ì£¼ëª©(attention)"í•˜ë©´ì„œ ì˜ë¯¸ë¥¼ íŒŒì•…

**ì´ê²Œ ë°”ë¡œ Self-Attentionì…ë‹ˆë‹¤.** 
RNNì²˜ëŸ¼ ìˆœì„œëŒ€ë¡œ ì½ì§€ ì•Šê³ , ëª¨ë“  ë‹¨ì–´ê°€ ë™ì‹œì— ì„œë¡œë¥¼ ë°”ë¼ë³´ëŠ” ê±°ì£ .

## ğŸ“Š ì„±ëŠ¥ì€ ì •ë§ ì¢‹ì•„ì¡Œë‚˜?

ë…¼ë¬¸ì˜ Table 2ë¥¼ ë³´ë©´ ì¶©ê²©ì ì…ë‹ˆë‹¤:

![ì„±ëŠ¥ ë¹„êµí‘œ]({{ site.baseurl }}/assets/images/performance-table.png)

- **Training Cost**: 10Â²~10Â³ë°° ì°¨ì´ (base ëª¨ë¸ ê¸°ì¤€)
- **BLEU Score**: ë‹¹ì‹œ ìµœê³  ê¸°ë¡ ê°±ì‹ 

**"ì™€, ì´ê±° ì§„ì§œë„¤?"** í–ˆë˜ ê¸°ì–µì´ ë‚˜ë„¤ìš”.
