---
layout: post
title: "왜 LLM API는 본질적으로 Stateless인가? (디자인 필연성과 실무 패턴)"
date: 2025-08-31 10:12:34 +0900
categories: [llm-engineering]
tags: [LLM, API설계, Stateless, 세션관리, 아키텍처]
redirect_from:
  - /llm-engineering/2025/08/31/llm-api-stateless/
---
# 왜 LLM API는 매번 대화 기록을 다시 보낼까: 무상태와 컨텍스트, 그리고 실전 최적화

저는 Openrouter를 사용해 챗봇 서비스를 개발하는 도중, 라우터가 프로바이더를 변경할 경우 어떻게 캐시와 컨텍스트가 유지되는지 궁금하더군요. 그런데 충격적이게도, 대부분의 LLM API는 **매번 전체 대화 기록을 클라이언트가 보내야** 한다는 사실을 알게 되었습니다.

이 방식은 겉보기에는 매우 비효율적으로 보입니다. 매번 텍스트가 길어질수록 토큰 비용이 늘어나고, 네트워크 대역폭도 낭비되죠. "서버가 기억하면 안 되나?"라는 생각이 들 수밖에 없습니다.

오늘은 이 문제를 파고들어봤습니다. 왜 2025년에도 우리는 이런 "비효율적인" 방식을 쓰고 있을까요? 그리고 현업에서는 이걸 어떻게 해결하고 있을까요?

## 무상태와 맥락의 간극: 수백만 사용자가 만든 필연

### 현대 웹의 규모: Stateful이 불가능한 이유

잠깐, 숫자를 한번 볼까요?

- ChatGPT 주간 활성 사용자: **2억 명**
- 일일 대화 수: **수십억 건**
- 동시 접속자: **수백만 명**

이제 상상해보세요. 만약 서버가 각 사용자의 대화를 **모두 메모리에 저장**한다면?

```python
# 단순 계산
사용자당_평균_대화 = 10KB
동시_접속자 = 1_000_000
필요_메모리 = 10KB * 1_000_000 = 10GB
```

그냥 단순히 텍스트만 계산한다고 생각해도 이렇죠. 그 이외에 각종 사용자 정보와 캐시 데이터까지 저장한다면? 서버가 감당 가능한 수준은 아니겠죠.

"그럼 분산 저장하면 되잖아?" 라고 생각하실 수 있습니다. 맞아요, Redis나 Memcached 같은 분산 캐시를 쓸 수 있죠. 하지만...

- 서버 재시작하면? → 모든 대화 증발
- 사용자가 다른 서버로 연결되면? → 대화 내역 없음
- 서버 A가 죽으면? → 해당 서버 사용자들 대화 전부 날아감
- 메모리 부족? → 3시간마다 서버 재시작...

또한 stateful한 아키텍쳐 도입으로 다음과 같은 문제들도 겪을 수 있을거에요.

**새로운 문제들:**
- 네트워크 지연 추가
- Redis 장애 = 전체 서비스 장애
- 동시 요청 처리 복잡도 ↑
- 세션 동기화 이슈

이제 우리는 왜 2025년에도 여전히 stateless한 HTTP를 메인 프로토콜로 쓰는지 이해할 수 있습니다.

### REST가 선택한 길: 무상태의 지혜

Roy Fielding이 REST를 설계할 때, 이미 이 문제를 예견했습니다. **"웹은 전 세계적 규모로 작동해야 한다"**

```
[Stateful 아키텍처]
Client A ←→ Server 1 (A의 상태 저장)
Client B ←→ Server 2 (B의 상태 저장)
Client C ←→ Server 3 (C의 상태 저장)
→ 서버 간 상태 공유 필요, 복잡도 폭발

[Stateless 아키텍처]
Client A (상태 포함) → 아무 서버나 OK
Client B (상태 포함) → 아무 서버나 OK  
Client C (상태 포함) → 아무 서버나 OK
→ 무한 확장 가능!
```

**무상태의 압도적 장점:**
- **무한 수평 확장**: 서버 100대든 1000대든 그냥 추가
- **완벽한 장애 격리**: 서버 하나 죽어도 다른 서버로
- **캐싱 용이**: CDN, 프록시 캐시 활용 가능
- **단순한 운영**: 세션 클러스터링? 그런 거 없음

### LLM API도 같은 선택: 규모의 경제학

OpenAI가 ChatGPT API를 설계할 때도 똑같은 고민을 했을 겁니다.

문제점:
- 2억 명의 세션을 어떻게 관리?
- 세션당 GPU 메모리 할당?
- 사용자가 1주일 뒤 돌아오면?


**현실적으로 불가능합니다.** 그래서 선택한 게 지금의 방식이죠:

```python
# 현재의 stateless API
messages = [전체_대화_내역]  # 클라이언트가 관리
response = openai.chat.completions.create(
    model="gpt-5",
    messages=messages  # 매번 전송
)
```

"비효율적이지만 확장 가능한" 트레이드오프입니다.

## KV 캐시의 진실: 왜 세션을 유지할 수 없는가

### GPU 메모리의 현실적 한계

"그래도 KV 캐시는 있잖아?" 맞습니다. 하지만 이건 결국 **규모와 컨텍스트 길이의 문제**입니다. KV 캐시는 토큰 수에 (거의) 선형으로 증가합니다. 7B~13B급 모델의 경우 공개된 분석들을 보면 토큰당 KV 캐시가 대략 수백 KB~1MB 수준이며, 13B 모델은 0.8~1.0MB/토큰 어림값이 자주 인용됩니다. 그러면 4K 컨텍스트만 유지해도 세션당 수 GB가 됩니다.

> 참고 자료: https://www.rohan-paul.com/p/kv-caching-in-llm-inference-a-comprehensive
참고 개념 (단순화):
- 한 토큰당 KV 용량 ≈ 2 × n_layers × n_heads × head_dim × dtype_bytes(FP16=2B)
- (예) Llama‑2‑7B ≈ ~0.5MB/토큰, 13B ≈ ~0.8MB/토큰 수준으로 정리된 바 있음

세션 유지에 따라 비용이 왜 비현실적일수 있는지를 적절한 페르미 추정을 통해 분석해봅시다:

```python
# A100 80GB 기준 (모델 가중치/프레임워크 오버헤드 제외한 'KV-only' 거친 추정)

# 시나리오 A: 짧은 컨텍스트 (~600 tokens @ ~0.8MB/token ≈ 0.5GB)
단일_세션_KV  ≈ 0.5  GB
동시_세션_최대 ≈ 80 / 0.5  ≈ 160 세션 / GPU

# 시나리오 B: 중간 컨텍스트 (~2K tokens ≈ 1.6GB)
단일_세션_KV  ≈ 1.6  GB
동시_세션_최대 ≈ 80 / 1.6  ≈ 50  세션 / GPU

# 시나리오 C: 긴 컨텍스트 (~4K tokens ≈ 3.2~4.0GB)
단일_세션_KV  ≈ 3.2~4.0 GB
동시_세션_최대 ≈ 80 / (3.2~4.0) ≈ 20~25 세션 / GPU
```

동시 1만 명이 "활성 컨텍스트를 점유"한다고 가정해 보죠:
- 시나리오 A: 10,000 / 160 ≈ 63대
- 시나리오 C: 10,000 / (20~25) ≈ 400~500대

즉 **컨텍스트 길이에 따라 필요한 GPU 대수가 수배 이상 벌어집니다.**

비용 관점(예시 수치, 시장 변동 있음):
- 구매가: A100 80GB 1대 ≈ $15k~$17k → 63대면 $0.95M~$1.07M
- 클라우드 온디맨드: 시간당 $5.1 정도 가정 시 63대 × $5.1 × 720h ≈ $232K/월 (플랫폼/약정/스팟 여부 따라 $1.x~$6 범위 흔함)

핵심 요지:
1. "세션당 KV 500MB"는 짧은 컨텍스트(수백 토큰)일 때에 불과하며 실사용자는 수천 토큰으로 쉽게 확대.
2. 2K→4K로 늘어날수록 세션당 수 GB가 되어 동시 세션 밀도가 급감.
3. 위 계산은 KV만 본 단순화; 실제는 모델 가중치, 런타임 워킹 메모리, 배치 스케줄링 오버헤드 등이 더해져 추가 여유(헤드룸)가 필요.
4. 그럼에도 불구하고 "KV 메모리는 토큰 길이에 선형"이라는 스케일링 법칙 때문에 **긴 컨텍스트를 서버가 장기 유지하는 설계는 비용 폭발**로 이어짐.

결론적으로 장기 세션을 GPU 메모리에 붙잡아 두려 하기보다:
- 요약(Summarization)으로 과거 프롬프트를 압축
- RAG로 외부 저장소(디스크/벡터 DB)에서 필요한 부분만 재주입
- 단기 TTL 캐시로 ‘활성’ 세션만 유지

와 같은 전략을 채택하는 방식을, 현재의 LLM 서비스들이 채택했다고 생각할 수 있겠네요.

## 최적화 방법 두가지

### 1. RAG: 분산 저장소 활용

세션 대신 **검색 가능한 저장소**를 활용합니다.

```python
# Stateless RAG 구현
def handle_request(user_id, message, conversation_id):
    # 1. 벡터 DB에서 관련 대화 검색 (Stateless)
    relevant_history = vectordb.search(
        query=message,
        filter={"conversation_id": conversation_id},
        top_k=5
    )
    
    # 2. 검색된 컨텍스트로 응답 생성
    response = llm.generate(
        context=relevant_history,
        query=message
    )
    
    # 3. 새 대화를 DB에 저장 (비동기)
    asyncio.create_task(
        vectordb.insert(message, response, conversation_id)
    )
    
    return response
```

**확장성 보장:**
- 벡터 DB는 수평 확장 가능
- 서버는 상태 유지 안 함
- 검색은 병렬 처리 가능

이 방법을 사용할 경우 RAG 기술의 장점을 생각해서 메모리 형태로 다른 날짜 혹은 다른 세션의 정보 또한 불러오는 식으로 활용할 수 있다는 차별화를 실현할 수 있습니다.

### 2. 하이브리드 아키텍처: 임시 세션 + Stateless

정말 필요한 경우에만 **제한적 세션**을 운영합니다. 생각해보면 채팅 기반 서비스를 운영하면서도 항상 세션을 유지할 필요가 없는 경우도 있어요!

```python
class HybridSessionManager:
    def __init__(self):
        self.cache = TTLCache(maxsize=10000, ttl=300)  # 5분 TTL
    
    def get_session(self, session_id):
        # 캐시에 있으면 사용
        if session_id in self.cache:
            return self.cache[session_id]
        
        # 없으면 클라이언트가 보낸 전체 컨텍스트 사용
        return None
    
    def handle_request(self, session_id, messages):
        cached = self.get_session(session_id)
        
        if cached:
            # 캐시 히트: 증분만 처리
            context = cached + messages[-1:]
        else:
            # 캐시 미스: 전체 처리
            context = messages
        
        response = llm.generate(context)
        
        # 단기 캐시 업데이트
        self.cache[session_id] = context
        
        return response
```

**균형점:**
- 활성 세션만 단기 캐시
- 5분 이상 비활성 → 자동 삭제
- 캐시 미스 시 → Stateless로 폴백

사용자가 오래 세션을 점유하지 않을것이 예측되는 서비스의 경우 이런 방법을 사용해 미세 최적화를 시도해 볼 수 있을것 같아요.

## 운영 현실: 스케일에서 살아남기

### 실제 프로덕션 아키텍처(예시)

```
[클라이언트]
- 전체 대화 내역 보관
- 필요시 압축/요약
↓
[API Gateway]
- Rate limiting
- Request 분산
↓
[Stateless LLM Workers] (N대)
- 수평 확장 가능
- 아무 워커나 처리 가능
- 장애 시 자동 페일오버
↓
[선택적 캐시 레이어]
- Redis: 활성 세션만
- Vector DB: 장기 기억
- CDN: 공통 응답
```

### 모니터링 필수 지표

아래 지표들을 주기적으로 수집하여 대시보드(예: Grafana)와 알림 시스템(예: PagerDuty, Slack)에 연동합니다.

- concurrent_users: 현재 활성(요청 발생 중 또는 최근 N초 내 활동) 사용자 수. 급증은 수평 확장 필요 신호.
- avg_context_size: 평균 프롬프트(컨텍스트) 길이. 모델 비용 및 지연(latency) 증가의 1차 지표.
- cache_hit_ratio: (캐시 적중 / 전체 요청). 낮아질 경우 캐시 전략(세션 TTL, chunking, 요약 정책) 재평가 필요.
- token_per_user: 사용자당 평균 토큰 소비. 가격 및 과금 전략 조정 판단 근거.
- server_memory: 워커 노드 메모리 사용률 %. 80% 이상 지속 시 OOM 및 swap 위험.
- gpu_utilization: GPU 활용률 (SM, 메모리, KV 캐시 점유). 저활용 시 배치 정책 / 고활용 시 모델 샤딩 또는 인스턴스 증설 검토.

알림 조건(예시 정책):
- server_memory > 80% (지속 5분): "메모리 부족! Stateless 전환(요약/프루닝) 또는 노드 증설 필요"
- concurrent_users > 10,000 (증가 속도 급격): "확장 필요! 오토스케일 조정"
- cache_hit_ratio < 0.30 (15분 평균): "캐시 비효율 — 세션 TTL 또는 요약 주기 점검"
- avg_context_size 급증(전일 대비 +25%): "프롬프트 누적 과다 — 자동 요약(Summarization) 트리거 검토"

운영 팁:
- 알림은 단일 임계치(threshold)보다 '지속 시간 + 증가 속도' 조건을 함께 사용(노이즈 감소).
- token_per_user 추세와 매출(또는 크레딧 소모) 그래프를 겹쳐 비용 탄력성 평가.
- gpu_utilization 편차가 큰 경우: 배치 크기, 동시 스트리밍 처리 정책, 프롬프트 길이 상한 재조정.

조사해본 결과 이런 지표들을 통해서 서비스의 상태를 지속적으로 모니터링하고, 필요시 알림을 받을 수 있는 체계를 구축하는 것이 중요하다는 결론을 얻었어요.

## 마무리: 규모가 만든 아키텍처

**"왜 LLM 챗봇은 매번 대화 기록을 다시 보낼까?"**

이제 진짜 답을 아시겠죠? 

**현대 웹의 규모에서는 Stateful이 사실상 불가능합니다.** ChatGPT처럼 2억 명이 쓰는 서비스가 모든 대화 상태를 서버 메모리에 저장한다? 기술적으로도, 경제적으로도 불가능한 얘기입니다.

그래서 우리는 "비효율적으로 보이는" Stateless를 선택합니다. 하지만 이게 정말 비효율적일까요?

- **무한 확장 가능** (서버 1000대도 OK)
- **완벽한 장애 복구** (어떤 서버가 죽어도 OK)
- **단순한 운영** (복잡한 동기화 불필요)
- **예측 가능한 비용** (사용한 만큼만 과금)

물론 토큰을 더 쓰긴 합니다. 하지만 **서버 1000대를 운영하는 복잡도**보다는 **토큰 비용이 싸다**는 게 업계의 결론입니다.

어떠한 기술적 결정을 하기 전에는 항상 그것에 대한 설명을 할 수 있어야 하듯이, 규모에 따라 아키텍쳐 구조 또한 적합하거나 적합하지 않아 질 수 있는거죠. 

그러니까 우리가 앞으로 서비스를 운영하려 한다면, 다음과 같은 원칙을 기억하면 좋을 것 같습니다.

**"스케일은 모든 아키텍처 결정을 지배한다"**