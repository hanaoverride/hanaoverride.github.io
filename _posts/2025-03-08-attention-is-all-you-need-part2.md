---
layout: post
title: "Attention Is All You Need ë…¼ë¬¸ : ì²˜ìŒ ì½ì–´ë³´ê¸°! - (2)"
date: 2025-03-08 19:38:18 +0900
categories: [llm-engineering]
tags: [Transformer, ë…¼ë¬¸ë¦¬ë·°, RNNí•œê³„, AttentionMechanism, ë”¥ëŸ¬ë‹]
redirect_from:
  - /llm-engineering/2025/03/08/attention-is-all-you-need-part2/
---
### **Part 2: "Transformer ëœ¯ì–´ë³´ê¸°" - í•µì‹¬ êµ¬ì¡° ì´í•´í•˜ê¸°**

## ğŸ—ï¸ ì „ì²´ êµ¬ì¡°: ë°˜ë³µë˜ëŠ” ë¸”ë¡
Figure 1: ì¸ì½”ë” 6, ë””ì½”ë” 6 (ê°™ì€ êµ¬ì¡° ë°˜ë³µ) â†’ ê¹Šì´ë¡œ í‘œí˜„ë ¥ í™•ë³´.

## ğŸ¯ Scaled Dot-Product Attention
ìˆ˜ì‹:
```
Attention(Q,K,V) = softmax(QK^T / âˆšd_k) V
```
í•µì‹¬: ìœ ì‚¬ë„(QÂ·K)ë¥¼ í™•ë¥ ë¡œ â†’ Value ê°€ì¤‘ í•©.

## ğŸ™ Multi-Head Attention
ì—¬ëŸ¬ "ê´€ì " ë³‘ë ¬ í•™ìŠµ â†’ ë¬¸ë²•/ì˜ë¯¸/ìœ„ì¹˜ ë“± ë‹¤ë¥¸ ì„œë¸ŒìŠ¤í˜ì´ìŠ¤.
Concat í›„ ì„ í˜•ë³€í™˜ìœ¼ë¡œ í†µí•©.

## ğŸ“ Positional Encoding
ìˆœì„œ ì •ë³´ ë¶€ì—¬ (sin, cos ì£¼ê¸°ì  íŒ¨í„´) â†’ ì ˆëŒ€+ìƒëŒ€ ìœ„ì¹˜ ëª¨ë‘ ê°„ì ‘ í•™ìŠµ.

## ğŸ”„ ì”ì°¨ ì—°ê²° & LayerNorm
Residual: ê¸°ìš¸ê¸° íë¦„ ê°œì„ .
LayerNorm: ë°°ì¹˜ í¬ê¸°ì™€ ë¬´ê´€í•˜ê²Œ ì•ˆì •í™”.

## ğŸ”Œ Feed-Forward (Position-wise)
ë‘ ë²ˆì˜ Linear + ReLU (ë…¼ë¬¸ì€ ReLU, êµ¬í˜„ì€ ì¢…ì¢… GELU).
ì°¨ì› í™•ì¥(d_modelâ†’4*d_model) í›„ ì¶•ì†Œ.

## ğŸ§± Encoder layer ìˆœì„œ
1. Multi-Head Self-Attention (+ ì”ì°¨ + LayerNorm)
2. Position-wise FFN (+ ì”ì°¨ + LayerNorm)

## ğŸ“¤ Decoder ì¶”ê°€ ìš”ì†Œ
1. Masked Self-Attention (ë¯¸ë˜ í† í° ê°€ë¦¼)
2. Encoder-Decoder Attention (ì†ŒìŠ¤ ë¬¸ì¥ê³¼ alignment)
3. FFN

## ğŸ›ï¸ Mask ì¢…ë¥˜
- Padding mask: íŒ¨ë”© í† í° ì˜í–¥ ì°¨ë‹¨
- Subsequent(mask): ë¯¸ë˜ ì •ë³´ ëˆ„ì¶œ ë°©ì§€

## ğŸ—œï¸ ì™œ Scaling í•„ìš”?
`QK^T` ê°’ ë¶„ì‚° ì»¤ì§ â†’ softmax í° ê°’ saturate â†’ gradient vanishing ìœ„í—˜ â†’ `1/âˆšd_k` ë¡œ ì•ˆì •í™”.

## âœ… ì •ë¦¬
- Attention = ë³‘ë ¬, ì¥ê¸° ì˜ì¡´ ìš©ì´
- Multi-Head = ë‹¤ì–‘í•œ ì˜ë¯¸ íˆ¬ì˜
- PE = ìˆœì„œ ì •ë³´ ì£¼ì…
- Residual+Norm = í•™ìŠµ ì•ˆì •ì„±

ë‹¤ìŒ í¸(3): í•™ìŠµ ì„¸ë¶€(optimizer, label smoothing, regularization) & ê²°ê³¼ ì§€í‘œ.
